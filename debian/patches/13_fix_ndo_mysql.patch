--- shinken/shinken/db.py
+++ shinken/shinken/db.py
@@ -131,3 +131,7 @@ class DB(object):
     def fetchone(self):
         """Just get an entry"""
         return self.db_cursor.fetchone()
+
+    def fetchall(self):
+        """Get all entry"""
+        return self.db_cursor.fetchall()
--- shinken/shinken/modules/ndodb_mysql_broker/ndodb_mysql_broker.py	2013-10-21 17:18:53.206463042 -0400
+++ shinken/shinken/modules/ndodb_mysql_broker/ndodb_mysql_broker.py	2013-10-21 17:20:07.637922930 -0400
@@ -25,12 +25,8 @@
 
 import copy
 import time
-
-properties = {
-    'daemons': ['broker'],
-    'type': 'ndodb_mysql',
-    'phases': ['running'],
-    }
+import sys
+from datetime import datetime
 
 try:
     import _mysql_exceptions
@@ -38,8 +34,35 @@
 except ImportError:
     _mysql_exceptions = None
 
-from shinken.basemodule import BaseModule
 from shinken.log import logger
+from shinken.basemodule import BaseModule
+
+
+# called by the plugin manager to get a instance
+def get_instance(mod_conf):
+
+    logger.debug("Get a ndoDB instance for plugin %s" % mod_conf.get_name())
+
+    if not _mysql_exceptions:
+        raise Exception('Cannot load module python-mysqldb. Please install it.')
+
+    # Default behavior: character_set is utf8 and synchro is turned off
+    if not hasattr(mod_conf, 'character_set'):
+        mod_conf.character_set = 'utf8'
+    if not hasattr(mod_conf, 'synchronize_database_id'):
+        mod_conf.synchronize_database_id = '1'
+    instance = Ndodb_Mysql_broker(mod_conf)
+
+    return instance
+
+
+properties = {
+    'daemons': ['broker'],
+    'type': 'ndodb_mysql',
+    'phases': ['running'],
+    }
+
+
 
 
 def de_unixify(t):
@@ -321,6 +344,28 @@
         else:
             return row[0]
 
+    def get_comments_internal_comment_id_by_obj_id_sync(self, obj_id, instance_id):
+        query = u"SELECT internal_comment_id from %scomments where " \
+                "object_id='%s' and instance_id='%s'" % \
+                (self.prefix, obj_id, instance_id)
+        self.db.execute_query(query)
+        rows = self.db.fetchall() # we need to modify db.py to nake it work
+        if rows is None or len(rows) < 1:
+            return []
+        else:
+            return [x[0] for x in rows]
+
+    def get_downtime_internal_downtime_id_by_obj_id_sync(self, obj_id, instance_id):
+        query = u"SELECT internal_downtime_id from %sscheduleddowntime where " \
+                "object_id='%s' and instance_id='%s'" % \
+                (self.prefix, obj_id, instance_id)
+        self.db.execute_query(query)
+        rows = self.db.fetchall() # we need to modify db.py to nake it work
+        if rows is None or len(rows) < 1:
+            return []
+        else:
+            return [x[0] for x in rows]
+
     def get_max_contactgroup_id_sync(self):
         query = u"SELECT COALESCE(max(contactgroup_id) + 1,1) from %scontactgroups" % self.prefix
         self.db.execute_query(query)
@@ -345,6 +390,8 @@
             'contactgroup_members', 'servicegroup_members',
             # Status tables
             'programstatus', 'hoststatus', 'servicestatus',
+            # Comment and downtime
+            'comments', 'scheduleddowntime', 'commenthistory', 'downtimehistory'
             ]
 
         res = []
@@ -521,7 +568,108 @@
 
         hoststatus_query = self.db.create_insert_query('hoststatus', hoststatus_data)
 
-        return [query, hoststatus_query]
+        query_list = [query, hoststatus_query]
+
+        ## Add comments
+        base_comment_ids = self.get_comments_internal_comment_id_by_obj_id_sync(host_id, data['instance_id'])
+        comments = data['comments']
+        host_comm_d = dict((c.id, c) for c in comments)
+
+        ids_to_add = [hci for hci in host_comm_d if hci not in base_comment_ids]
+        ids_to_del = [bci for bci in base_comment_ids if bci not in host_comm_d]
+
+        for c_id in ids_to_del:
+            query_move = u"INSERT INTO %scommenthistory " \
+                         "(instance_id, entry_time, entry_time_usec, comment_type, entry_type, object_id, " \
+                         "comment_time, internal_comment_id, author_name, comment_data, is_persistent, " \
+                         "comment_source, expires, expiration_time, deletion_time, deletion_time_usec)"\
+                         " SELECT instance_id, entry_time, entry_time_usec, comment_type, entry_type," \
+                         " object_id, comment_time, internal_comment_id, author_name, comment_data," \
+                         " is_persistent, comment_source, expires, expiration_time, NOW(), 0" \
+                         "  FROM %scomments WHERE internal_comment_id='%s';" % \
+                         (self.prefix, self.prefix, c_id)
+
+            query_del = u"DELETE FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, c_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for c_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(host_comm_d[c_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_exptime = datetime.fromtimestamp(host_comm_d[c_id].expire_time).strftime('%Y-%m-%d %H:%M:%S')
+            comment_data = {
+                'instance_id': data['instance_id'],
+                'entry_time': mysql_enttime,
+                'entry_time_usec': 0,
+                'comment_type': host_comm_d[c_id].comment_type,
+                'entry_type': host_comm_d[c_id].entry_type,
+                'object_id': host_id,
+                'comment_time': mysql_enttime,  # NDO doc make a small difference
+                'internal_comment_id': c_id,
+                'author_name': host_comm_d[c_id].author,
+                'comment_data': host_comm_d[c_id].comment,
+                'is_persistent': host_comm_d[c_id].persistent,
+                'comment_source': host_comm_d[c_id].source,
+                'expires': host_comm_d[c_id].expires,
+                'expiration_time': mysql_exptime,
+            }
+
+            query_add = self.db.create_insert_query('comments', comment_data)
+            query_list.append(query_add)
+
+        ## Add downtime
+        base_ids = self.get_downtime_internal_downtime_id_by_obj_id_sync(host_id, data['instance_id'])
+        downtimes = data['downtimes']
+        dict_ids = dict((d.id, d) for d in downtimes)
+
+        ids_to_add = [hci for hci in dict_ids if hci not in base_ids]
+        ids_to_del = [bci for bci in base_ids if bci not in dict_ids]
+
+        for dt_id in ids_to_del:
+            query_move = u"INSERT INTO %sdowntimehistory " \
+                         "(instance_id, downtime_type, object_id, entry_time, author_name, comment_data, " \
+                         "internal_downtime_id, triggered_by_id, is_fixed, duration, scheduled_start_time, " \
+                         "scheduled_end_time, was_started,  actual_start_time, " \
+                         "actual_start_time_usec, actual_end_time, actual_end_time_usec, was_cancelled)"\
+                         " SELECT instance_id, downtime_type, object_id, entry_time, author_name," \
+                         " comment_data, internal_downtime_id, triggered_by_id, is_fixed, duration," \
+                         " scheduled_start_time, scheduled_end_time, was_started,  actual_start_time," \
+                         " actual_start_time_usec, NOW(), 0, IF(scheduled_end_time > NOW(), 1, 0)" \
+                         " FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % \
+                         (self.prefix, self.prefix, dt_id)
+
+            query_del = u"DELETE FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % (self.prefix, dt_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for dt_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(dict_ids[dt_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_strtime = datetime.fromtimestamp(dict_ids[dt_id].start_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_endtime = datetime.fromtimestamp(dict_ids[dt_id].end_time).strftime('%Y-%m-%d %H:%M:%S')
+            data = {
+                'instance_id': data['instance_id'],
+                'downtime_type': 2,
+                'object_id': host_id,
+                'entry_time': mysql_enttime,
+                'author_name': dict_ids[dt_id].author,
+                'comment_data': dict_ids[dt_id].comment,
+                'internal_downtime_id': dt_id,
+                'triggered_by_id': dict_ids[dt_id].trigger_id,
+                'is_fixed': dict_ids[dt_id].fixed,
+                 'duration': dict_ids[dt_id].duration,
+                'scheduled_start_time': mysql_strtime,
+                'scheduled_end_time': mysql_endtime,
+                'was_started': dict_ids[dt_id].is_in_effect,
+                'actual_start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
+                'actual_start_time_usec': 0
+
+            }
+
+            query_add = self.db.create_insert_query('scheduleddowntime', data)
+            query_list.append(query_add)
+
+        return query_list
 
     # A service have just been created, database is clean, we INSERT it
     def manage_initial_service_status_brok(self, b):
@@ -630,7 +778,109 @@
 
         servicestatus_query = self.db.create_insert_query('servicestatus', servicestatus_data)
 
-        return [query, servicestatus_query]
+        query_list = [query, servicestatus_query]
+
+        ## Add comments
+        base_comment_ids = \
+            self.get_comments_internal_comment_id_by_obj_id_sync(service_id, data['instance_id'])
+        # TODO check if we this is comment obj.
+        comments = data['comments']
+        svc_comm_d = dict((c.id, c) for c in comments)
+
+        ids_to_add = [hci for hci in svc_comm_d if hci not in base_comment_ids]
+        ids_to_del = [bci for bci in base_comment_ids if bci not in svc_comm_d]
+
+        for c_id in ids_to_del:
+            query_move = u"INSERT INTO %scommenthistory " \
+                         "(instance_id, entry_time, entry_time_usec, comment_type, entry_type, object_id, " \
+                         "comment_time, internal_comment_id, author_name, comment_data, is_persistent, " \
+                         "comment_source, expires, expiration_time, deletion_time, deletion_time_usec)"\
+                         " SELECT instance_id, entry_time, entry_time_usec, comment_type, entry_type," \
+                         " object_id, comment_time, internal_comment_id, author_name, comment_data," \
+                         " is_persistent, comment_source, expires, expiration_time, NOW(), 0" \
+                         "  FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, self.prefix, c_id)
+
+            query_del = u"DELETE FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, c_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for c_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(svc_comm_d[c_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_exptime = datetime.fromtimestamp(svc_comm_d[c_id].expire_time).strftime('%Y-%m-%d %H:%M:%S')
+            comment_data = {
+                'instance_id': data['instance_id'],
+                'entry_time': mysql_enttime,
+                'entry_time_usec': 0,
+                'comment_type': svc_comm_d[c_id].comment_type,
+                'entry_type': svc_comm_d[c_id].entry_type,
+                'object_id': service_id,
+                'comment_time': mysql_enttime,  # NDO doc make a small difference
+                'internal_comment_id': c_id,
+                'author_name': svc_comm_d[c_id].author,
+                'comment_data': svc_comm_d[c_id].comment,
+                'is_persistent': svc_comm_d[c_id].persistent,
+                'comment_source': svc_comm_d[c_id].source,
+                'expires': svc_comm_d[c_id].expires,
+                'expiration_time': mysql_exptime,
+            }
+
+            query_add = self.db.create_insert_query('comments', comment_data)
+            query_list.append(query_add)
+
+        ## Add downtime
+        base_ids = self.get_downtime_internal_downtime_id_by_obj_id_sync(service_id, data['instance_id'])
+        downtimes = data['downtimes']
+        dict_ids = dict((d.id, d) for d in downtimes)
+
+        ids_to_add = [hci for hci in dict_ids if hci not in base_ids]
+        ids_to_del = [bci for bci in base_ids if bci not in dict_ids]
+
+        for dt_id in ids_to_del:
+            query_move = u"INSERT INTO %sdowntimehistory " \
+                         "(instance_id, downtime_type, object_id, entry_time, author_name, comment_data, " \
+                         "internal_downtime_id, triggered_by_id, is_fixed, duration, scheduled_start_time, " \
+                         "scheduled_end_time, was_started,  actual_start_time, " \
+                         "actual_start_time_usec, actual_end_time, actual_end_time_usec, was_cancelled)"\
+                         " SELECT instance_id, downtime_type, object_id, entry_time, author_name," \
+                         " comment_data, internal_downtime_id, triggered_by_id, is_fixed, duration," \
+                         " scheduled_start_time, scheduled_end_time, was_started,  actual_start_time," \
+                         " actual_start_time_usec, NOW(), 0, IF(scheduled_end_time > NOW(), 1, 0)" \
+                         " FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % \
+                         (self.prefix, self.prefix, dt_id)
+
+            query_del = u"DELETE FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % (self.prefix, dt_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for dt_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(dict_ids[dt_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_strtime = datetime.fromtimestamp(dict_ids[dt_id].start_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_endtime = datetime.fromtimestamp(dict_ids[dt_id].end_time).strftime('%Y-%m-%d %H:%M:%S')
+            data = {
+                'instance_id': data['instance_id'],
+                'downtime_type': 1,
+                'object_id': service_id,
+                'entry_time': mysql_enttime,
+                'author_name': dict_ids[dt_id].author,
+                'comment_data': dict_ids[dt_id].comment,
+                'internal_downtime_id': dt_id,
+                'triggered_by_id': dict_ids[dt_id].trigger_id,
+                'is_fixed': dict_ids[dt_id].fixed,
+                'duration': dict_ids[dt_id].duration,
+                'scheduled_start_time': mysql_strtime,
+                'scheduled_end_time': mysql_endtime,
+                'was_started': dict_ids[dt_id].is_in_effect,
+                'actual_start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
+                'actual_start_time_usec': 0
+
+            }
+
+            query_add = self.db.create_insert_query('scheduleddowntime', data)
+            query_list.append(query_add)
+
+        return query_list
 
     # A new host group? Insert it
     # We need to do something for the members prop (host.id, host_name)
@@ -1005,7 +1255,108 @@
 
         hoststatus_query = self.db.create_update_query('hoststatus', hoststatus_data, where_clause)
 
-        return [query, hoststatus_query]
+        query_list = [query, hoststatus_query]
+
+        ## Add comments
+        base_comment_ids = self.get_comments_internal_comment_id_by_obj_id_sync(host_id, data['instance_id'])
+        # TODO check if we this is comment obj.
+        comments = data['comments']
+        host_comm_d = dict((c.id, c) for c in comments)
+
+        ids_to_add = [hci for hci in host_comm_d if hci not in base_comment_ids]
+        ids_to_del = [bci for bci in base_comment_ids if bci not in host_comm_d]
+
+        for c_id in ids_to_del:
+            query_move = u"INSERT INTO %scommenthistory " \
+                         "(instance_id, entry_time, entry_time_usec, comment_type, entry_type, object_id, " \
+                         "comment_time, internal_comment_id, author_name, comment_data, is_persistent, " \
+                         "comment_source, expires, expiration_time, deletion_time, deletion_time_usec)"\
+                         " SELECT instance_id, entry_time, entry_time_usec, comment_type, entry_type," \
+                         " object_id, comment_time, internal_comment_id, author_name, comment_data," \
+                         " is_persistent, comment_source, expires, expiration_time, NOW(), 0" \
+                         "  FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, self.prefix, c_id)
+
+            query_del = u"DELETE FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, c_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for c_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(host_comm_d[c_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_exptime = datetime.fromtimestamp(host_comm_d[c_id].expire_time).strftime('%Y-%m-%d %H:%M:%S')
+            comment_data = {
+                'instance_id': data['instance_id'],
+                'entry_time': mysql_enttime,
+                'entry_time_usec': 0,
+                'comment_type': host_comm_d[c_id].comment_type,
+                'entry_type': host_comm_d[c_id].entry_type,
+                'object_id': host_id,
+                'comment_time': mysql_enttime,  # NDO doc make a small difference
+                'internal_comment_id': c_id,
+                'author_name': host_comm_d[c_id].author,
+                'comment_data': host_comm_d[c_id].comment,
+                'is_persistent': host_comm_d[c_id].persistent,
+                'comment_source': host_comm_d[c_id].source,
+                'expires': host_comm_d[c_id].expires,
+                'expiration_time': mysql_exptime,
+            }
+
+            query_add = self.db.create_insert_query('comments', comment_data)
+            query_list.append(query_add)
+
+        ## Add downtime
+        base_ids = self.get_downtime_internal_downtime_id_by_obj_id_sync(host_id, data['instance_id'])
+        downtimes = data['downtimes']
+        dict_ids = dict((d.id, d) for d in downtimes)
+
+        ids_to_add = [hci for hci in dict_ids if hci not in base_ids]
+        ids_to_del = [bci for bci in base_ids if bci not in dict_ids]
+
+        for dt_id in ids_to_del:
+            query_move = u"INSERT INTO %sdowntimehistory " \
+                         "(instance_id, downtime_type, object_id, entry_time, author_name, comment_data, " \
+                         "internal_downtime_id, triggered_by_id, is_fixed, duration, scheduled_start_time, " \
+                         "scheduled_end_time, was_started,  actual_start_time, " \
+                         "actual_start_time_usec, actual_end_time, actual_end_time_usec, was_cancelled)"\
+                         " SELECT instance_id, downtime_type, object_id, entry_time, author_name," \
+                         " comment_data, internal_downtime_id, triggered_by_id, is_fixed, duration," \
+                         " scheduled_start_time, scheduled_end_time, was_started,  actual_start_time," \
+                         " actual_start_time_usec, NOW(), 0, IF(scheduled_end_time > NOW(), 1, 0)" \
+                         " FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % \
+                         (self.prefix, self.prefix, dt_id)
+
+            query_del = u"DELETE FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % (self.prefix, dt_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for dt_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(dict_ids[dt_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_strtime = datetime.fromtimestamp(dict_ids[dt_id].start_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_endtime = datetime.fromtimestamp(dict_ids[dt_id].end_time).strftime('%Y-%m-%d %H:%M:%S')
+            data = {
+                'instance_id': data['instance_id'],
+                'downtime_type': 2,
+                'object_id': host_id,
+                'entry_time': mysql_enttime,
+                'author_name': dict_ids[dt_id].author,
+                'comment_data': dict_ids[dt_id].comment,
+                'internal_downtime_id': dt_id,
+                'triggered_by_id': dict_ids[dt_id].trigger_id,
+                'is_fixed': dict_ids[dt_id].fixed,
+                 'duration': dict_ids[dt_id].duration,
+                'scheduled_start_time': mysql_strtime,
+                'scheduled_end_time': mysql_endtime,
+                'was_started': dict_ids[dt_id].is_in_effect,
+                'actual_start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
+                'actual_start_time_usec': 0
+
+            }
+
+            query_add = self.db.create_insert_query('scheduleddowntime', data)
+            query_list.append(query_add)
+
+        return query_list
 
     # Ok the service is updated
     def manage_update_service_status_brok(self, b):
@@ -1088,12 +1439,114 @@
 
         where_clause = {'service_object_id': service_id}
         servicestatus_query = self.db.create_update_query(
-            'servicestatus', \
-            servicestatus_data, \
+            'servicestatus',
+            servicestatus_data,
             where_clause
-            )
+        )
+
+        query_list = [query, servicestatus_query]
+
+        ## Add comments
+        base_comment_ids = \
+            self.get_comments_internal_comment_id_by_obj_id_sync(service_id, data['instance_id'])
+        # TODO check if we this is comment obj.
+        comments = data['comments']
+        svc_comm_d = dict((c.id, c) for c in comments)
+
+        ids_to_add = [hci for hci in svc_comm_d if hci not in base_comment_ids]
+        ids_to_del = [bci for bci in base_comment_ids if bci not in svc_comm_d]
+
+        for c_id in ids_to_del:
+            query_move = u"INSERT INTO %scommenthistory " \
+                         "(instance_id, entry_time, entry_time_usec, comment_type, entry_type, object_id, " \
+                         "comment_time, internal_comment_id, author_name, comment_data, is_persistent, " \
+                         "comment_source, expires, expiration_time, deletion_time, deletion_time_usec)"\
+                         " SELECT instance_id, entry_time, entry_time_usec, comment_type, entry_type," \
+                         " object_id, comment_time, internal_comment_id, author_name, comment_data," \
+                         " is_persistent, comment_source, expires, expiration_time, NOW(), 0" \
+                         "  FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, self.prefix, c_id)
+
+            query_del = u"DELETE FROM %scomments WHERE internal_comment_id='%s';" % (self.prefix, c_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for c_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(svc_comm_d[c_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_exptime = datetime.fromtimestamp(svc_comm_d[c_id].expire_time).strftime('%Y-%m-%d %H:%M:%S')
+            comment_data = {
+                'instance_id': data['instance_id'],
+                'entry_time': mysql_enttime,
+                'entry_time_usec': 0,
+                'comment_type': svc_comm_d[c_id].comment_type,
+                'entry_type': svc_comm_d[c_id].entry_type,
+                'object_id': service_id,
+                'comment_time': mysql_enttime,  # NDO doc make a small difference
+                'internal_comment_id': c_id,
+                'author_name': svc_comm_d[c_id].author,
+                'comment_data': svc_comm_d[c_id].comment,
+                'is_persistent': svc_comm_d[c_id].persistent,
+                'comment_source': svc_comm_d[c_id].source,
+                'expires': svc_comm_d[c_id].expires,
+                'expiration_time': mysql_exptime,
+            }
+
+            query_add = self.db.create_insert_query('comments', comment_data)
+            query_list.append(query_add)
+
+        ## Add downtime
+        base_ids = self.get_downtime_internal_downtime_id_by_obj_id_sync(service_id, data['instance_id'])
+        downtimes = data['downtimes']
+        dict_ids = dict((d.id, d) for d in downtimes)
+
+        ids_to_add = [hci for hci in dict_ids if hci not in base_ids]
+        ids_to_del = [bci for bci in base_ids if bci not in dict_ids]
+
+        for dt_id in ids_to_del:
+            query_move = u"INSERT INTO %sdowntimehistory " \
+                         "(instance_id, downtime_type, object_id, entry_time, author_name, comment_data, " \
+                         "internal_downtime_id, triggered_by_id, is_fixed, duration, scheduled_start_time, " \
+                         "scheduled_end_time, was_started,  actual_start_time, " \
+                         "actual_start_time_usec, actual_end_time, actual_end_time_usec, was_cancelled)"\
+                         " SELECT instance_id, downtime_type, object_id, entry_time, author_name," \
+                         " comment_data, internal_downtime_id, triggered_by_id, is_fixed, duration," \
+                         " scheduled_start_time, scheduled_end_time, was_started,  actual_start_time," \
+                         " actual_start_time_usec, NOW(), 0, IF(scheduled_end_time > NOW(), 1, 0)" \
+                         " FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % \
+                         (self.prefix, self.prefix, dt_id)
+
+            query_del = u"DELETE FROM %sscheduleddowntime WHERE internal_downtime_id='%s';" % (self.prefix, dt_id)
+
+            query_list.append(query_move)
+            query_list.append(query_del)
+
+        for dt_id in ids_to_add:
+            mysql_enttime = datetime.fromtimestamp(dict_ids[dt_id].entry_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_strtime = datetime.fromtimestamp(dict_ids[dt_id].start_time).strftime('%Y-%m-%d %H:%M:%S')
+            mysql_endtime = datetime.fromtimestamp(dict_ids[dt_id].end_time).strftime('%Y-%m-%d %H:%M:%S')
+            data = {
+                'instance_id': data['instance_id'],
+                'downtime_type': 1,
+                'object_id': service_id,
+                'entry_time': mysql_enttime,
+                'author_name': dict_ids[dt_id].author,
+                'comment_data': dict_ids[dt_id].comment,
+                'internal_downtime_id': dt_id,
+                'triggered_by_id': dict_ids[dt_id].trigger_id,
+                'is_fixed': dict_ids[dt_id].fixed,  #Check is boolean is well insert
+                 'duration': dict_ids[dt_id].duration,
+                'scheduled_start_time': mysql_strtime,
+                'scheduled_end_time': mysql_endtime,
+                'was_started': dict_ids[dt_id].is_in_effect,
+                'actual_start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
+                'actual_start_time_usec': 0
+
+            }
+
+            query_add = self.db.create_insert_query('scheduleddowntime', data)
+            query_list.append(query_add)
 
-        return [query, servicestatus_query]
+        return query_list
 
     # A host have just be create, database is clean, we INSERT it
     def manage_initial_contact_status_brok(self, b):
